"""
Takes inputs from create_messenger_regions.py and outputs 2d array files for
the solar wind, magnetosheath, and magnetosphere.
"""

from pathlib import Path

import astropy.units as u
import numpy as np
import xarray as xr
from astropy.table import QTable
from hermpy.utils import Constants
from numpy.typing import NDArray

from determine_messenger_regions import RESOLUTION

MESSENGER_REGIONS_FILE = (
    Path(__file__).parent.parent / "resources/messenger_regions.ecsv"
)
OUTPUT_FILE = Path(__file__).parent.parent / "resources/region_probability_maps.nc"

INPUT_CADENCE = RESOLUTION.to("minute").value

# Settings for the probability map
BIN_SIZE = 0.25  # radii
X_BOUNDS = (-5, 5)
CYL_BOUNDS = (0, 8)


def main():

    # Load messenger regions, generated by src/create_messenger_regions.py
    messenger_regions = QTable.read(MESSENGER_REGIONS_FILE, format="ascii.ecsv")

    # Create grid
    x_bins = np.arange(X_BOUNDS[0], X_BOUNDS[1] + BIN_SIZE, BIN_SIZE)
    cyl_bins = np.arange(CYL_BOUNDS[0], CYL_BOUNDS[1] + BIN_SIZE, BIN_SIZE)

    region_probability_maps = {}

    region_names = np.unique(messenger_regions["Region"])
    for region_name in region_names:

        # Select only regions of this type
        region_observations = messenger_regions[
            messenger_regions["Region"] == region_name
        ]

        # We only need positions at this point
        # We also convert to radii
        region_observations = region_observations[["X MSM'", "CYL MSM'"]]
        region_observations["X MSM'"] = region_observations[
            "X MSM'"
        ] / Constants.MERCURY_RADIUS.to(u.km)
        region_observations["CYL MSM'"] = region_observations[
            "CYL MSM'"
        ] / Constants.MERCURY_RADIUS.to(u.km)

        # Create histogram of observations in each bin
        region_spatial_counts, _, _ = np.histogram2d(
            region_observations["X MSM'"],
            region_observations["CYL MSM'"],
            bins=[x_bins, cyl_bins],
        )

        region_probability_maps.update({region_name: region_spatial_counts})

    # To make these observation maps actually probability maps, we need to
    # divide each bin by the sum across all maps.
    map_totals = np.sum(
        [region_probability_maps[region] for region in region_names], axis=0
    )

    for region in region_names:
        region_probability_maps[region] /= map_totals

    # Save map with metadata as an xarray dataset
    region_probability_dataset = xr.Dataset()

    # Save bin centres as coords of the dataset
    region_probability_dataset.coords["X MSM'"] = (x_bins[:-1] + x_bins[1:]) / 2
    region_probability_dataset.coords["CYL MSM'"] = (cyl_bins[:-1] + cyl_bins[1:]) / 2

    for region_name, map in region_probability_maps.items():

        region_probability_dataset[region_name] = (("X MSM'", "CYL MSM'"), map)

    region_probability_dataset["Minutes In Bin"] = (
        ("X MSM'", "CYL MSM'"),
        map_totals * INPUT_CADENCE,
    )  # map_totals is in units of measurement cadence

    region_probability_dataset["N Observations"] = (("X MSM'", "CYL MSM'"), map_totals)

    region_probability_dataset = determine_proportional_confidence_interval(
        region_probability_dataset
    )

    # Save as NetCDF
    region_probability_dataset.to_netcdf(OUTPUT_FILE)

    # Check that it was saved correctly
    loaded = xr.load_dataset(OUTPUT_FILE)
    print(loaded)


def determine_proportional_confidence_interval(
    probabilitiy_map: xr.Dataset,
) -> xr.Dataset:

    probabilitiy_map = probabilitiy_map.copy()

    region_names = ["Solar Wind", "Magnetosheath", "Magnetosphere"]
    for region in region_names:

        totals = probabilitiy_map["N Observations"].values
        successes = probabilitiy_map[region].values * totals

        lower, upper = adjusted_wald_interval(successes, totals)

        probabilitiy_map[f"{region} 95% Lower"] = (("X MSM'", "CYL MSM'"), lower)
        probabilitiy_map[f"{region} 95% Upper"] = (("X MSM'", "CYL MSM'"), upper)

    return probabilitiy_map


def adjusted_wald_interval(
    n_success: NDArray[np.float64], n_samples: NDArray[np.float64]
) -> tuple[NDArray[np.float64], NDArray[np.float64]]:
    """
    Get the 95% confidence interval for a Bernoulli trial process with n_sucess
    and n_samples.
    """

    z = 1.96

    p_hat = (n_success + z) / (n_samples + z**2)

    std_error = np.sqrt(p_hat * (1 - p_hat) / n_samples)

    upper = p_hat + 1.96 * std_error
    lower = p_hat - 1.96 * std_error

    # This is a little counter intuitive. These functions
    # return the max / min between two arrays.
    lower = np.maximum(lower, 0)
    upper = np.minimum(upper, 1)

    return lower, upper


if __name__ == "__main__":
    main()
